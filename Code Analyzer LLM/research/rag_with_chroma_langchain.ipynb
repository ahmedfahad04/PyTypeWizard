{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Simple RAG using ChromaDB and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source [Medium](https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document loaders form [Langchain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genAi_python_type_inference.pdf  \u001b[0m\u001b[01;32mpyty_paper.pdf\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to your pdf files:\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "  \"\"\"\n",
    "    Load PDF documents from the specified directory using PyPDFDirectoryLoader.\n",
    "    \n",
    "    Returns:\n",
    "    List of Document objects: Loaded PDF documents represented as Langchain Document objects.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Initialize PDF loader with specified directory\n",
    "  document_loader = PyPDFDirectoryLoader(DATA_PATH) \n",
    "  # Load PDF documents and return them as a list of Document objects\n",
    "  return document_loader.load() \n",
    "\n",
    "documents = load_documents() # Call the function\n",
    "\n",
    "# Inspect the contents of the first document as well as metadata\n",
    "# print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "In RAG systems, “chunking” refers to the segmentation of input text into shorter and more meaningful units. This enables the system to efficiently pinpoint and retrieve relevant pieces of information. The quality of chunks are **essential** to how effective your system will be.\n",
    "\n",
    "The most **important** thing to consider when **deciding a chunking strategy** is the structure of the documents that you are loading into your vector database. If the documents contain similar-length paragraphs, it would be useful to consider this when determining the size of the chunk.\n",
    "\n",
    "----\n",
    "\n",
    "If your `chunk_size` is too large, then you will be inputing a lot of noisy and unwanted context to the final LLM query. Further, as LLMs are limited by their context window size, the larger the chunk, the fewer pieces of relevant information you can include as context to your query.\n",
    "\n",
    "The `chunk_overlap` refers to intentionally duplicating tokens at the beginning and end of each adjacent chunk. This helps retain additional context from neighbouring chunks which can improve the quality of the information passed into the prompt.\n",
    "\n",
    "When deploying these systems to real-world applications, it is important to plot distributions of text lengths in your data, and tune these parameters based on experimentation of parameters such as `chunk_size` and `chunk_overlap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "  \"\"\"\n",
    "  Split the text content of the given list of Document objects into smaller chunks.\n",
    "  \n",
    "  Args:\n",
    "    documents (list[Document]): List of Document objects containing text content to split.\n",
    "  Returns:\n",
    "    list[Document]: List of Document objects representing the split text chunks.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Initialize text splitter with specified parameters\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  # Split documents into smaller chunks using text splitter\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "  # Print example of page content and metadata for a chunk\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks # Return the list of split text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VectorDB (Chroma)\n",
    "\n",
    "It's encourageg experimentation with open-sourced models such as **Llama3** (try the 8B parameter version first, especially if your system needs to be 100% local). If you are working on a very niche or nuanced use case, off-the-shelf embedding models may not be useful. Therefore, you might want to investigate fine-tuning the embedding model on the domain data to improve the retrieval quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to save Chroma database\n",
    "CHROMA_PATH = \"chromaDB\"\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "  \"\"\"\n",
    "  Save the given list of Document objects to a Chroma database.\n",
    "  \n",
    "  Args:\n",
    "  chunks (list[Document]): List of Document objects representing text chunks to save.\n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "  \n",
    "  load_dotenv()\n",
    "  GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "  # Clear out the existing database directory if it exists\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "  # Create a new Chroma database from the documents using GEMINI embeddings\n",
    "  db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY),\n",
    "    persist_directory=CHROMA_PATH\n",
    "  )\n",
    "\n",
    "  # Persist the database to disk\n",
    "  db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 25 documents into 733 chunks.\n",
      "Generative Type Inference for Python\n",
      "Yun Peng†, Chaozheng Wang†, Wenxuan Wang†, Cuiyun Gao‡, Michael R. Lyu†\n",
      "†Computer Science and Engineering Department, The Chinese University of Hong Kong, Hong Kong, China\n",
      "‡School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China\n",
      "{'source': 'data/genAi_python_type_inference.pdf', 'page': 0, 'start_index': 0}\n",
      "Saved 733 chunks to chromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153330/237340530.py:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "def generate_data_store():\n",
    "  \"\"\"\n",
    "  Function to generate vector database in chroma from documents.\n",
    "  \"\"\"\n",
    "  documents = load_documents() # Load documents from a source\n",
    "  chunks = split_text(documents) # Split documents into manageable chunks\n",
    "  save_to_chroma(chunks) # Save the processed data to a data store\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Generate the data store\n",
    "generate_data_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"How Generative AI is used to infer python types?\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "{context}\n",
    "However this context might contain incomplete statements. So understand the content inherent context to provide insightful answers\n",
    "\n",
    "====\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text):\n",
    "  \"\"\"\n",
    "  Query a Retrieval-Augmented Generation (RAG) system using Chroma database and OpenAI.\n",
    "  Args:\n",
    "    - query_text (str): The text to query the RAG system with.\n",
    "    \n",
    "  Returns:\n",
    "    - formatted_response (str): Formatted response including the generated text and sources.\n",
    "    - response_text (str): The generated response text.\n",
    "  \"\"\"\n",
    "  \n",
    "  load_dotenv()\n",
    "  GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "  \n",
    "  # YOU MUST - Use same embedding function as before\n",
    "  embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "  # Prepare the database\n",
    "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "  \n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "  \n",
    "  print(\"CONTEXT TEXT: \\n\", context_text)\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "  \n",
    "  # Initialize OpenAI chat model\n",
    "  model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.5, max_tokens=1048, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = model.invoke(prompt)  # model.predict() but its depricated\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"\\nResponse: {response_text}\\nSources: {sources}\"\n",
    "  return formatted_response, response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      "CONTEXT TEXT: \n",
      " GPT-Neo 1.3B/2.7B The Pile Generative\n",
      "InCoder 1.3B/6.7BGitHub &\n",
      "Stack OverflowGenerative\n",
      "& Infilling\n",
      "CodeGen 6BThe Pile, BigQuery\n",
      "& GitHubGenerative\n",
      "GPT-J 6.7B The Pile Generative\n",
      "GPT-3.5 175B - Generative\n",
      "ChatGPT 175B - Generative\n",
      "•Type4Py [27] is a supervised type inference approach. It\n",
      "\n",
      " - -\n",
      "\n",
      "ChatGPT 175B - Generative\n",
      "•Type4Py [27] is a supervised type inference approach. It\n",
      "builds different type clusters and classifies a new Python\n",
      "program into one of the type clusters to determine the\n",
      "type.\n",
      "•HiTyper [34] is a hybrid type inference approach. It\n",
      "\n",
      " - -\n",
      "\n",
      "ative type inference approach for Python programs. T YPEGEN\n",
      "has four phases, including code slicing, type hint collection,\n",
      "chain-of-thought (COT) prompt construction, and type gen-\n",
      "eration. In the code slicing phase, T YPEGENgenerates type\n",
      "dependency graphs (TDGs) and builds code slices based on\n",
      "content=\"The provided context highlights several approaches to type inference in Python, including:\\n\\n* **Type4Py:** A supervised approach that classifies programs into type clusters.\\n* **HiTyper:** A hybrid approach (details not provided).\\n* **TYPEGEN:** A generative approach that utilizes a chain-of-thought (COT) prompt construction to generate types.\\n\\n**Here's how Generative AI is used in TYPEGEN:**\\n\\n1. **Code Slicing:** TYPEGEN analyzes the code to create type dependency graphs (TDGs) and code slices.\\n2. **Type Hint Collection:** It gathers existing type hints from the code.\\n3. **COT Prompt Construction:**  It constructs a prompt that guides the generative model to reason about the code and its types.\\n4. **Type Generation:**  The generative model uses the prompt and code context to predict the types of variables and functions.\\n\\n**In essence, TYPEGEN leverages the ability of generative AI to understand code context and generate text (in this case, type information) based on that understanding.** \\n\" response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-4080bcc1-0263-44ed-ac67-b8ff154cd72b-0' usage_metadata={'input_tokens': 317, 'output_tokens': 218, 'total_tokens': 535}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(query_text)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
