{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Simple RAG using ChromaDB and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source [Medium](https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: chardet in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: tabulate in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (0.5.7)\n",
      "Requirement already satisfied: python-iso639 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (3.10.0)\n",
      "Requirement already satisfied: backoff in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (0.26.1)\n",
      "Requirement already satisfied: wrapt in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (6.0.0)\n",
      "Requirement already satisfied: python-oxmsg in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured) (0.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from dataclasses-json->unstructured) (3.21.3)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from dataclasses-json->unstructured) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from nltk->unstructured) (2024.5.15)\n",
      "Requirement already satisfied: olefile in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from requests->unstructured) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from requests->unstructured) (2024.7.4)\n",
      "Requirement already satisfied: cryptography>=3.1 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (43.0.3)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (0.2.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.9.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (2.9.2)\n",
      "Requirement already satisfied: pypdf>=4.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->unstructured) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client->unstructured) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /home/fahad/miniconda3/envs/ptenv/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document loaders form [Langchain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytagfix'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = 'Testing-Repositories/New Repo/pytagfix'\n",
    "db_name = repo_path.split('/')[-1]\n",
    "db_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Custom LanguageParser to catch decoding errors and report the problematic file\n",
    "class DebugLanguageParser(LanguageParser):\n",
    "    def lazy_parse(self, blob):\n",
    "        try:\n",
    "            # Attempt to read the file content\n",
    "            code = blob.as_string()\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Print the file causing the issue and the error details\n",
    "            print(f\"Encoding error in file: {blob.source}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            return  # Skip this blob due to the encoding issue\n",
    "\n",
    "        language = self.language or (\n",
    "            LANGUAGE_EXTENSIONS.get(blob.source.rsplit(\".\", 1)[-1])\n",
    "            if isinstance(blob.source, str)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if language is None:\n",
    "            raise ValueError(\"Could not determine the language for the provided blob.\")\n",
    "\n",
    "        # Create a Document object manually\n",
    "        yield Document(page_content=code, metadata={\"source\": blob.source})\n",
    "        \n",
    "        \n",
    "def load_documents(document_path):\n",
    "    \n",
    "\n",
    "    # Use the custom parser to catch encoding issues\n",
    "    document_loader = GenericLoader.from_filesystem(\n",
    "        document_path,\n",
    "        glob=\"**/*\",\n",
    "        suffixes=[\".py\"],\n",
    "        parser=DebugLanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    "    )\n",
    "    \n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = load_documents(repo_path)\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "In RAG systems, “chunking” refers to the segmentation of input text into shorter and more meaningful units. This enables the system to efficiently pinpoint and retrieve relevant pieces of information. The quality of chunks are **essential** to how effective your system will be.\n",
    "\n",
    "The most **important** thing to consider when **deciding a chunking strategy** is the structure of the documents that you are loading into your vector database. If the documents contain similar-length paragraphs, it would be useful to consider this when determining the size of the chunk.\n",
    "\n",
    "----\n",
    "\n",
    "If your `chunk_size` is too large, then you will be inputing a lot of noisy and unwanted context to the final LLM query. Further, as LLMs are limited by their context window size, the larger the chunk, the fewer pieces of relevant information you can include as context to your query.\n",
    "\n",
    "The `chunk_overlap` refers to intentionally duplicating tokens at the beginning and end of each adjacent chunk. This helps retain additional context from neighbouring chunks which can improve the quality of the information passed into the prompt.\n",
    "\n",
    "When deploying these systems to real-world applications, it is important to plot distributions of text lengths in your data, and tune these parameters based on experimentation of parameters such as `chunk_size` and `chunk_overlap`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing `PythonCodeTextSplitter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    \n",
    "    splitter = PythonCodeTextSplitter(\n",
    "        chunk_size=1000,    # Each chunk will be 500 characters\n",
    "        chunk_overlap=100   # Overlap of 50 characters between chunks\n",
    "    )\n",
    "\n",
    "    # Split documents into smaller chunks using text splitter\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Print example of page content and metadata for a chunk\n",
    "    document = chunks[0]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks # Return the list of split text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VectorDB (Chroma)\n",
    "\n",
    "It's encourageg experimentation with open-sourced models such as **Llama3** (try the 8B parameter version first, especially if your system needs to be 100% local). If you are working on a very niche or nuanced use case, off-the-shelf embedding models may not be useful. Therefore, you might want to investigate fine-tuning the embedding model on the domain data to improve the retrieval quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to save Chroma database\n",
    "CHROMA_PATH = db_name\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    \"\"\"\n",
    "    Save the given list of Document objects to a Chroma database.\n",
    "\n",
    "    Args:\n",
    "    chunks (list[Document]): List of Document objects representing text chunks to save.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "    # Ensure API key is loaded correctly\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google API key not found in environment variables\")\n",
    "\n",
    "    # Clear out the existing database directory if it exists\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)  # Ensure all cached files are deleted\n",
    "        \n",
    "        while os.path.exists(CHROMA_PATH):  # Extra safeguard to ensure deletion\n",
    "            pass\n",
    "\n",
    "    \n",
    "    os.mkdir(CHROMA_PATH)  # Ensure directory is created if not exists\n",
    "\n",
    "    # Create a new Chroma database from the documents using GEMINI embeddings\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        db = Chroma.from_documents(\n",
    "            chunks,\n",
    "            GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY),\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            # client_settings={\"use_existing_db\": False}  # Ensure a new database is created\n",
    "        )\n",
    "        # db.persist()  # Ensure the data is written properly\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save to Chroma: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Loaded!\n",
      "Split 2 documents into 16 chunks.\n",
      "import re\n",
      "{'source': 'Testing-Repositories/New Repo/pytagfix/prettytitle.py'}\n",
      "Coverted Document into Chunks!\n",
      "Saved 16 chunks to pytagfix.\n"
     ]
    }
   ],
   "source": [
    "def generate_data_store():\n",
    "  \"\"\"\n",
    "  Function to generate vector database in chroma from documents.\n",
    "  \"\"\"\n",
    "  documents = load_documents(repo_path) # Load documents from a source\n",
    "  print(\"Document Loaded!\")\n",
    "  \n",
    "  chunks = split_text(documents) # Split documents into manageable chunks\n",
    "  print(\"Coverted Document into Chunks!\")\n",
    "  \n",
    "  save_to_chroma(chunks) # Save the processed data to a data store\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Generate the data store\n",
    "generate_data_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Give all method signatures. Add proper type hints to arguments and return types\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "{context}\n",
    "However this context might contain incomplete statements. So understand the content inherent context to provide insightful answers\n",
    "\n",
    "====\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text):\n",
    "  \"\"\"\n",
    "  Query a Retrieval-Augmented Generation (RAG) system using Chroma database and OpenAI.\n",
    "  Args:\n",
    "    - query_text (str): The text to query the RAG system with.\n",
    "    \n",
    "  Returns:\n",
    "    - formatted_response (str): Formatted response including the generated text and sources.\n",
    "    - response_text (str): The generated response text.\n",
    "  \"\"\"\n",
    "  \n",
    "  load_dotenv()\n",
    "  GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "  \n",
    "  # YOU MUST - Use same embedding function as before\n",
    "  embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "  # Prepare the database\n",
    "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "  \n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.5:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "  \n",
    "  print(\"CONTEXT TEXT: \\n\", context_text)\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "  \n",
    "  # Initialize OpenAI chat model\n",
    "  model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.5, max_tokens=1048, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = model.invoke(prompt)  # model.predict() but its depricated\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"\\nResponse: {response_text}\\nSources: {sources}\"\n",
    "  return formatted_response, response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      "CONTEXT TEXT: \n",
      " #!/usr/bin/python\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import taglib\n",
      "\n",
      "from prettytitle import pretty_string\n",
      "\n",
      "\n",
      "def process_dir(srcdir):\n",
      "    mf = []\n",
      "    for dirpath, dirs, files in os.walk(srcdir, topdown=False):\n",
      "        for name in sorted(files):\n",
      "            try:\n",
      "                ff = taglib.File(os.path.join(os.path.abspath(dirpath), name))\n",
      "            except:\n",
      "                print(\"SKIPPING not music file: {0}\".format(name))\n",
      "            else:\n",
      "                mf.append(ff)\n",
      "    process_music_files(mf)\n",
      "\n",
      "\n",
      "def input_yes(s):\n",
      "    ans = input(\"{} [y/N]\".format(s))\n",
      "    if ans and (ans in \"yY\"):\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "\n",
      "def get_album_artist(artist_list):\n",
      "    print(\"More then one artist found\")\n",
      "    if len(artist_list) < 5:\n",
      "        if input_yes(\"Is it Split\"):\n",
      "            return \" & \".join(artist_list)\n",
      "    else:\n",
      "        if input_yes(\"Is it VA\"):\n",
      "            return \"VA\"\n",
      "    return None\n",
      "\n",
      " - -\n",
      "\n",
      "if len(newdata[tagname]) > 1:\n",
      "            print(\"More then one {0} found:\".format(tagname))\n",
      "            print(\"    \" + \"\\n    \".join(newdata[tagname]))\n",
      "            newdata[tagname] = [input(\"Enter {0}: \".format(tagname))]\n",
      "    if len(newdata[\"DATE\"][0]) != 4:\n",
      "        print(\"Date doesn't look like a year number: \" + newdata[\"DATE\"][0])\n",
      "        newdate = input(\n",
      "            \"Enter date or press enter to use {0}:\".format(newdata[\"DATE\"][0])\n",
      "        )\n",
      "        if newdate:\n",
      "            newdata[\"DATE\"] = [newdate]\n",
      "    return newdata\n",
      "\n",
      " - -\n",
      "\n",
      "RUSCAP_RE = (\n",
      "    \"сатан([аыеу]|ой)?\",\n",
      "    \"дьявол([ауе]|ом)?\",\n",
      "    \"христ(ос)?([ауе]|ом)?\",\n",
      "    \"иисус([ауе]|ом)?\",\n",
      "    \"тьм([аыеу]|ой|ою)\",\n",
      "    \"хаос([аеу]|ом)?\",\n",
      "    \"пустот([аыу]|ой)\",\n",
      ")\n",
      "\n",
      "CYRILLIC_LETTERS = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\n",
      "CONSONANTS = \"BCDFGHJKLMNPQRSTVWXZ\" + \"БВЖЗЙКЛМНПРСТФХЦЧШЩ\"\n",
      "PLUSES = \"ЪЬ-'`\"\n"
     ]
    }
   ],
   "source": [
    "formatted_response, response_text = query_rag(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(response_text.pretty_print()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output2.md', 'w+') as f:\n",
    "    f.writelines(response_text.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
